# Mini VAE Interpolation

A PyTorch implementation of a **Variational Autoencoder (VAE)** trained on **MNIST** digits, with smooth interpolation capabilities in the latent space.

---

## ğŸ§  Model Architecture

- **Encoder**: 3 convolutional layers  *(Channels: 32 â†’ 64 â†’ 128, with BatchNorm)*
- **Latent Space**: 20-dimensional with **reparameterization trick**
- **Decoder**: 3 transposed convolutional layers to reconstruct 28Ã—28 images
- **Loss Function**: `Binary Cross-Entropy + KL Divergence (Î²-VAE variant)`

---

---

## ğŸ“Š Results

### ğŸ” Reconstructions  
**Original digits (top row)** vs **VAE reconstructions (bottom row)**:

![Reconstructions](static/reconstructions.png)

---

### ğŸ”„ Interpolations  
Smooth transitions between different digits:

- **0 to 1 Interpolation**  
  ![0 to 1 Interpolation](static/interpolation_0_to_1.gif)

- **2 to 7 Interpolation**  
  ![2 to 7 Interpolation](static/interpolation_2_to_7.gif)
